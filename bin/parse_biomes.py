#!/usr/bin/env python3

import argparse
import pandas as pd
import os
import re

def parse_meta_b(meta_b_df):
    counts = {}

    for index, row in meta_b_df.iterrows():
        if (isinstance(row['biomes'], str)): # avoiding unexpected nan values
            row_nums = re.sub(r'[\[\]]', '', row['biomes']).split(',')
            for i in range(0, len(row_nums), 2):
                biome_id = int(row_nums[i])
                counts[biome_id] = counts.get(biome_id, 0) + 1

    return counts

def count_per_family(file_path):
    meta_b_df = pd.read_csv(file_path, usecols=[1], sep='\t', header=None, names=['biomes'])
    counts = parse_meta_b(meta_b_df)

    counts_df = pd.DataFrame(list(counts.items()), columns=['id', 'count'])

    return counts_df

def translate_counts(counts_df, biome_mapping_df):
    counts_df = pd.merge(counts_df, biome_mapping_df, on='id', how='left')

    return counts_df

def calculate_parent_counts(counts_df):
    parent_counts = {}

    for index, row in counts_df.iterrows():
        name_components = row['name'].split(':')
        parent_name = ''
        
        for component in name_components:
            parent_name += component
            parent_counts[parent_name] = parent_counts.get(parent_name, 0) + row['count']
            parent_name += ':'

    parent_counts_df = pd.DataFrame(list(parent_counts.items()), columns=['name', 'count'])
    parent_counts_df = parent_counts_df.sort_values(by='count', ascending=False)

    return parent_counts_df

def get_parent(biome_path):
    parts = biome_path.split(':')
    if len(parts) <= 1:
        return ''
    parent = ':'.join(parts[:-1])
    return parent

def get_label(biome_path):
    parts = biome_path.split(':')
    if len(parts) == 0:
        return ''  # No label found
    return parts[-1]

def write_out(parent_counts_df, out_file):
    with open(out_file, 'w') as out_file:
        out_file.write("ids,labels,parents,counts\n")
        for index, row in parent_counts_df.iterrows():
            grandparent_name = get_parent(row['name'])
            label = get_label(row['name'])
            out_file.write(f"{row['name']},{label},{grandparent_name},{row['count']}\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Parse the protein query TSV results into biome sunburst count CSVs.")
    parser.add_argument("postprocessing_dir", help="Path to the output post-processing files generated by the query_mgnprotein_db module")
    # python3 bin/post-processing/parse_biomes.py /home/vangelis/Desktop/Projects/mgnifams/output/post-processing
    args = parser.parse_args()

    query_results_dir = os.path.join(args.postprocessing_dir, "query_results")
    if not os.path.exists(query_results_dir):
        raise FileNotFoundError(f"The folder {query_results_dir} does not exist.")

    biome_mapping_path = os.path.join(args.postprocessing_dir, "biome_mapping.tsv")
    if not os.path.exists(biome_mapping_path):
        raise FileNotFoundError(f"The file {biome_mapping_path} does not exist.")

    biome_mapping_df = pd.read_csv(biome_mapping_path, sep='\t', header=None, names=['id', 'name'])

    query_results_files = os.listdir(query_results_dir)

    outdir = "biome_results"
    os.makedirs(outdir, exist_ok=True)
    for tsv in query_results_files:
        file_path = os.path.join(query_results_dir, tsv)
        counts_df = count_per_family(file_path)
        counts_df = translate_counts(counts_df, biome_mapping_df)
        parent_counts_df = calculate_parent_counts(counts_df)

        basename = os.path.splitext(os.path.basename(tsv))[0]
        out_file = os.path.join(outdir, f"{basename}.csv")
        write_out(parent_counts_df, out_file)
